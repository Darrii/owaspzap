\documentclass[conference]{IEEEtran}
\usepackage{booktabs} % For professional tables
\usepackage{graphicx} % For including figures

\begin{document}

\title{Graph-Based Probabilistic Approach for Automated Vulnerability Chain Detection in Web Security Scanning}

\author{\IEEEauthorblockN{Dariga Yermakhankyzy, Syed Imran Moazzam Shah}
\IEEEauthorblockA{School of Information Technology and Engineering\\ Kazakh-British Technical University\\
Almaty, Kazakhstan\\
Email: d\_yermakhankyzy@kbtu.kz }
}

\maketitle

\begin{abstract}
Web application vulnerability scanners identify security weaknesses in isolation, failing to detect compound attack scenarios where multiple vulnerabilities combine to enable critical breaches. This paper presents a novel graph-based probabilistic approach for automated detection of multi-stage vulnerability chains in web applications. Our system models vulnerabilities as nodes in a directed graph and applies 53 domain-specific probabilistic rules to create edges representing attack transitions. A depth-first search algorithm explores the graph to identify chains of length 2-4, applying intelligent filtering to reduce tens of thousands of raw candidates to unique, actionable attack patterns. The system integrates seamlessly with OWASP ZAP through REST API, enabling real-time chain analysis within existing security workflows. Experimental evaluation on three deliberately vulnerable applications (DVWA, Juice Shop, WebGoat) demonstrates effectiveness across 842 vulnerabilities, identifying 37 unique attack chains invisible to traditional scanners. The system achieves 99.5-99.9\% deduplication rates and processes chains at 10-675 per second, with total analysis completing within 16 minutes even for applications generating over 300,000 graph edges. Key innovations include: (1) a configurable probabilistic rule engine with bounded boosting and precompiled regex optimization; (2) normalized 0-100 risk scoring combining severity, exploitability, chain length, and confidence; (3) on-the-fly pattern-based deduplication achieving 99\% reduction in analyst workload. Detected chains represent realistic attack scenarios including XSS-enabled CSRF bypasses and configuration weaknesses facilitating SQL injection, demonstrating practical value for prioritizing remediation efforts in production environments.
\end{abstract}

\begin{IEEEkeywords}
Web application security, vulnerability chain detection, graph-based analysis, probabilistic modeling, OWASP ZAP, attack path analysis, security testing automation, compound vulnerabilities
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}

Web applications have become critical infrastructure for modern organizations, handling sensitive data and business-critical operations across e-commerce, healthcare, finance, and government sectors. The increasing complexity of these applications has introduced numerous security vulnerabilities that pose significant risks to data integrity, confidentiality, and availability. Recent studies show that approximately 3.73\% of tested applications exhibit at least one instance of broken access control vulnerabilities alone \cite{OWASP2025}. Despite substantial organizational investments in security testing, detecting and remediating web application vulnerabilities remains challenging due to the sophisticated nature of modern cyber attacks.

Current web application security testing relies heavily on automated vulnerability scanners such as OWASP ZAP \cite{OWASPZAP}, Burp Suite, and Nikto. These tools employ static analysis, dynamic analysis, and fuzzing techniques to identify security weaknesses in web applications \cite{Makino2015, Srinivasan2017}. Comparative studies have demonstrated that modern vulnerability scanners can effectively detect individual vulnerabilities including SQL injection, cross-site scripting (XSS), and authentication flaws \cite{Potti2025, Mohaidat2024}. However, these scanners operate under a fundamental limitation: they identify vulnerabilities in isolation, treating each security issue as an independent finding without considering potential interactions or dependencies between multiple vulnerabilities.

This isolated detection approach fails to capture a critical threat vector in web security: multi-stage attack chains. In real-world scenarios, attackers rarely exploit a single vulnerability to compromise a system. Instead, sophisticated attacks combine multiple vulnerabilities in sequence, where each vulnerability enables the exploitation of the next, ultimately leading to critical security breaches that individual vulnerability assessments fail to predict \cite{Kasturi2024Priority}. For instance, an attacker might leverage information disclosure to obtain session tokens, exploit XSS to bypass CSRF protection, and finally achieve administrative privilege escalation—a three-stage attack chain that appears as three isolated medium-severity findings in traditional scan reports. Security analysts must manually review hundreds of vulnerability findings and mentally correlate them to identify such compound threats, a process that is time-consuming, error-prone, and requires significant expertise \cite{Kumar2017}.

Existing research has explored various approaches to address vulnerability correlation and attack path analysis. Statistical correlation methods have been proposed to identify relationships between vulnerabilities based on temporal and spatial patterns \cite{Kasturi2023Understanding}. Attack tree modeling provides theoretical frameworks for representing multi-stage attacks \cite{Mauw2006}, and recent advances in automated penetration testing leverage machine learning and reinforcement learning techniques \cite{Li2024DynPen, Deng2024Pentest}. However, these approaches suffer from several limitations: statistical methods require offline manual analysis and lack real-time processing capabilities; attack tree frameworks remain primarily theoretical without automated implementation for web applications; and ML-based penetration testing tools focus on network-level or IoT scenarios rather than web-specific vulnerability chains \cite{Rak2022}. Furthermore, none of these solutions provide seamless integration with widely-adopted vulnerability scanners, creating a gap between vulnerability detection and chain analysis.

This paper presents a graph-based probabilistic approach for automated detection of multi-stage attack chains in web applications, integrated with OWASP ZAP. Our system addresses existing limitations through four key contributions: a probabilistic rule engine with 53 domain-specific chain rules encoding real-world attack patterns with confidence scores; a graph-based detection algorithm using depth-first search to identify vulnerability chains of length 2-4 with sub-second performance; an intelligent filtering mechanism that reduces tens of thousands of potential chains to actionable findings through on-the-fly deduplication and subchain removal; and real-time integration with OWASP ZAP via REST API, enabling automated chain analysis immediately upon scan completion. In our experimental evaluation, the system analyzed 129 vulnerabilities and identified 44 critical attack chains in 0.4 seconds, reducing 37,000 raw chain candidates to actionable security insights.

\section{Literature Review}

The detection of security vulnerabilities in web applications has been extensively studied from multiple perspectives. We organize related work into four primary research areas: web vulnerability scanners and testing methodologies, vulnerability correlation and prioritization approaches, attack modeling and chain detection techniques, and machine learning applications in security analysis.

\subsection{Web Vulnerability Scanners and Testing Methodologies}

Automated vulnerability scanners constitute the foundation of modern web application security testing. OWASP ZAP (Zed Attack Proxy) represents one of the most widely adopted open-source security testing tools, offering both passive and active scanning capabilities \cite{OWASPZAP}. Recent benchmarking studies have evaluated ZAP's effectiveness in detecting OWASP Top 10 vulnerabilities, demonstrating its ability to identify SQL injection, XSS, and security misconfigurations with varying degrees of accuracy \cite{Potti2025}. Comparative analyses of vulnerability scanners including ZAP, Burp Suite, Arachni, and Nikto have revealed that while each tool exhibits unique strengths, all share a common limitation: they detect vulnerabilities independently without correlating findings \cite{Makino2015, Mohaidat2024, Srinivasan2017}.

Automated testing methodologies have evolved to address specific vulnerability classes. Combinatorial testing approaches have been proposed for SQL injection detection, systematically generating attack vectors based on input grammars \cite{Bozic2019}. Studies comparing automated versus manual penetration testing have demonstrated that while automation improves efficiency, manual analysis remains necessary for identifying complex attack scenarios \cite{Khan2020}. Recent advances in automated penetration testing include expert systems that guide security assessments through threat intelligence \cite{Granata2024} and LLM-enhanced tools that leverage large language models for test case generation \cite{Deng2024Pentest}. However, these approaches focus on individual vulnerability detection rather than identifying relationships between multiple security weaknesses. Furthermore, existing scanners lack context understanding between vulnerabilities, treating each finding as an isolated security issue without considering how multiple flaws might be chained together in a compound attack.

\subsection{Vulnerability Correlation and Prioritization}

The challenge of correlating vulnerability data from multiple sources has gained increasing attention in application security research. Statistical correlation methods have been developed to analyze relationships between vulnerability detection results and real-world attack patterns observed in Web Application Firewall (WAF) logs \cite{Kasturi2023Understanding}. These approaches employ time-series analysis to identify correlations between vulnerability types and actual exploitation attempts, providing insights into which combinations of vulnerabilities represent elevated risk \cite{Kasturi2024Priority}.

Attack path prediction represents an advanced application of vulnerability correlation. Recent work has proposed using correlation analysis combined with attack tree modeling and multi-layer perceptrons to predict likely attack sequences based on vulnerability distributions across application layers \cite{Kasturi2024Predicting}. This research demonstrates that vulnerabilities can be mapped to attack trees representing threat models, enabling simulation of potential attack paths. However, these correlation approaches require manual analysis and offline processing, lacking the real-time processing capability necessary for integration into security scanning workflows. Additionally, while statistical correlation can identify historical patterns, it does not incorporate domain knowledge about vulnerability exploitability and real-world attack probabilities.

Application vulnerability correlation (AVC) tools have emerged in the commercial security space to aggregate and normalize findings from multiple security testing tools \cite{Kasturi2024Priority}. These solutions address the problem of duplicate vulnerabilities reported by different scanners and attempt to prioritize remediation efforts. However, they focus primarily on deduplication and risk scoring of individual vulnerabilities rather than identifying multi-stage attack chains.

\subsection{Attack Modeling and Chain Detection}

Theoretical frameworks for modeling attacks have been established through attack tree and attack graph methodologies. Attack trees provide a formal notation for representing how attackers might achieve specific goals through combinations of actions \cite{Mauw2006}. This hierarchical representation captures attack sequences and alternative paths, enabling security analysts to reason about system vulnerabilities systematically. However, attack tree construction typically requires manual effort and domain expertise, limiting their application in automated security testing.

Threat modeling approaches have been developed for specific domains, particularly IoT and edge computing environments \cite{Ficco2021}. Expert systems for automated threat modeling and penetration testing have shown promise in IoT ecosystems, combining threat intelligence with automated testing frameworks \cite{Rak2022}. These systems demonstrate the feasibility of automating security assessment through rule-based approaches. Nevertheless, they are focused on network-level and IoT-specific threats rather than web application vulnerability chains.

Recent advances in automated penetration testing have explored reinforcement learning and deep learning techniques. Deep reinforcement learning has been applied to automated penetration testing in dynamic network scenarios, learning optimal attack paths through interaction with target systems \cite{Li2024DynPen}. LLM-empowered penetration testing tools have demonstrated the potential of large language models to automate complex security testing workflows \cite{Deng2023GPT}. While these approaches show promising results in network penetration testing, they are not specialized for web application chains and require substantial computational resources for training and execution.

\subsection{Machine Learning and Deep Learning for Vulnerability Detection}

Machine learning techniques have been increasingly applied to vulnerability detection and security analysis. Deep learning-based systems have been developed for source code vulnerability detection, employing neural networks to identify security flaws in software \cite{Li2018Vul, Li2022SySeVR}. Systematic literature reviews have examined the application of deep learning to web application security, finding that convolutional neural networks, long short-term memory networks, and deep feedforward networks are commonly used for vulnerability detection \cite{Alaoui2022Deep}.

Specific vulnerability classes have been targeted with machine learning approaches. LSTM encoder-decoder architectures with word embeddings have been proposed for XSS attack detection \cite{Alaoui2023XSS}. These models learn patterns from training data to classify inputs as benign or malicious. However, machine learning approaches to web security face significant limitations: they are trained on individual vulnerability patterns rather than attack sequences, require substantial labeled training data, and produce results that lack interpretability for security practitioners \cite{Chughtai2024}. Furthermore, ML-based vulnerability detection focuses on identifying instances of known vulnerability types rather than discovering relationships between multiple vulnerabilities that could form attack chains.

\subsection{Research Gaps}

Despite extensive research in web application security testing, several critical gaps remain unaddressed. First, existing vulnerability scanners detect security issues in isolation without identifying multi-stage attack chains that represent compound threats. Second, vulnerability correlation approaches require manual analysis and offline processing, lacking integration with real-time scanning tools. Third, attack modeling frameworks remain primarily theoretical or focused on non-web domains such as IoT and network security. Fourth, machine learning approaches target individual vulnerability detection rather than attack sequence identification. Our work addresses these gaps by introducing a probabilistic graph-based system that automatically detects vulnerability chains through real-time integration with OWASP ZAP, combining domain knowledge encoded in probabilistic rules with efficient graph traversal algorithms.

\section{Methods}

\subsection{Research Gaps}

Contemporary web application security assessment faces four fundamental challenges that limit vulnerability detection and remediation effectiveness.

\textbf{Gap 1: Isolated Vulnerability Detection.} Existing vulnerability scanners including OWASP ZAP, Burp Suite, and Nikto identify individual security weaknesses without analyzing relationships between findings \cite{Makino2015, Mohaidat2024}. When a scanner reports 100-200 vulnerabilities in a typical assessment, security analysts must manually review each finding independently. This approach misses compound attacks where multiple vulnerabilities combine to enable critical breaches. For example, an information disclosure vulnerability leaking authentication tokens becomes significantly more dangerous when combined with a CSRF bypass, yet scanners report these as separate medium-severity issues rather than identifying the critical attack chain they form.

\textbf{Gap 2: Manual Correlation Analysis.} Security analysts currently perform vulnerability correlation manually, examining hundreds of findings to identify potential attack paths \cite{Kumar2017}. This manual approach is time-consuming, requiring hours or days for comprehensive analysis of large scan reports, and error-prone, as analysts may overlook subtle relationships. Without automated correlation, critical multi-stage attack chains may remain undetected until exploited by attackers with the time and expertise to identify compound threats.

\textbf{Gap 3: Absence of Probability Assessment.} While statistical correlation methods have been proposed \cite{Kasturi2023Understanding}, they fail to incorporate domain knowledge about vulnerability exploitability and real-world attack probabilities. Existing approaches treat all vulnerability combinations equally, without considering that certain chains (e.g., XSS leading to CSRF bypass) are significantly more likely to be exploitable than others. This results in either overwhelming analysts with false positive chain candidates or missing genuine attack paths due to overly conservative filtering.

\textbf{Gap 4: Integration Challenges.} Most research tools for vulnerability correlation operate as standalone systems disconnected from the scanning workflow \cite{Kasturi2024Priority}. OWASP ZAP, despite being one of the most popular open-source security scanners, lacks built-in chain detection. This gap forces security teams to export scan results, manually process them through separate correlation tools, and reconcile findings across multiple systems—a workflow that introduces delays and reduces the likelihood of systematic chain analysis.

\subsection{Proposed Approach}

We propose a graph-based probabilistic approach that addresses these gaps through automated, real-time vulnerability chain detection integrated with OWASP ZAP. Our system models vulnerabilities and their relationships as a directed graph, where nodes represent security findings and edges encode probabilistic rules defining how one vulnerability can enable exploitation of another.

The approach consists of four interconnected components. A \textbf{graph representation} transforms vulnerability scan results into a structured format where each vulnerability becomes a node annotated with severity, exploitability, and context information. A \textbf{probabilistic rule engine} encodes 53 domain-specific chain rules capturing real-world attack patterns, such as "XSS enables CSRF bypass with 85\% probability" or "SQL injection leads to privilege escalation with 90\% probability." Each rule specifies source and target vulnerability types, transition probability, and contextual constraints for chain viability.

A \textbf{depth-first search algorithm} traverses the vulnerability graph to identify potential attack chains of length 2-4, applying probability thresholds to filter unlikely paths. The algorithm employs path pruning to avoid exploring chains with cumulative probability below a configurable threshold, maintaining computational efficiency for large vulnerability sets. A \textbf{smart filtering mechanism} processes raw chain candidates to eliminate duplicates and remove subchains—if chain A$\rightarrow$B$\rightarrow$C is detected, the system removes the shorter chain A$\rightarrow$B since it represents a subset of the longer path.

The system integrates with OWASP ZAP through its REST API, enabling automated chain analysis upon scan completion. When a ZAP scan finishes, our system retrieves vulnerability findings, constructs the graph, executes chain detection, and generates an enriched report highlighting critical attack chains alongside individual vulnerabilities. This integration eliminates the workflow gap between vulnerability detection and chain analysis, ensuring compound threats are identified as part of standard security assessment.

\subsection{System Architecture}

Figure 1 presents the complete system architecture, illustrating the eight-stage pipeline from vulnerability scanning to chain detection and reporting. The architecture follows a modular design where each component performs a specific transformation on the data flow.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/diagrams/figure_1_architecture.png}
\caption{System Architecture: Eight-stage vulnerability chain detection pipeline showing data flow from OWASP ZAP scanning through graph construction, rule application, DFS detection, and intelligent filtering to final report generation.}
\label{fig:architecture}
\end{figure}

The pipeline begins with OWASP ZAP scanning the target web application using spider-based crawling and active payload injection. The scanner produces a JSON report containing vulnerability alerts with metadata including severity, CWE classification, and affected URLs. Our ZAP Alert Parser processes this report, extracting vulnerability instances and performing signature-based deduplication to eliminate redundant findings (e.g., the same XSS vulnerability detected on multiple similar endpoints).

The Graph Builder constructs a NetworkX directed graph where each unique vulnerability becomes a node. The Probabilistic Rule Engine then applies 53 domain-specific chain rules to create edges between nodes. Each rule encodes attack patterns such as "XSS enables CSRF bypass" with an associated probability reflecting the likelihood that an attacker could exploit the first vulnerability to leverage the second. The rule engine incorporates optimizations: precompiled regular expressions for URL pattern matching (10-100$\times$ speedup), taxonomy-based caching for vulnerability classification (1291$\times$ speedup), and bounded boosting with a configurable multiplier cap (default 2.5$\times$) to prevent probability overflow while allowing context-aware adjustments.

The DFS Chain Detector performs depth-first search traversal of the vulnerability graph to identify all potential attack chains of length 2-4. The algorithm applies configurable constraints including minimum chain probability (default 0.65), maximum chains per source node (500), and maximum unique patterns globally (100) to balance comprehensive detection with computational efficiency. During traversal, the system performs on-the-fly deduplication using hash-based signatures, reducing tens of thousands of raw chain candidates to unique attack patterns.

The Smart Filter component applies two additional reduction stages. First, it performs final deduplication to ensure each unique vulnerability sequence appears only once, retaining the instance with the highest risk score when duplicates exist. Second, it removes subchains—if a longer chain A$\rightarrow$B$\rightarrow$C is detected, the system eliminates shorter chains A$\rightarrow$B and B$\rightarrow$C since they represent subsets of the longer attack path. This subchain removal focuses analyst attention on maximal attack scenarios rather than fragmentary intermediate steps.

Finally, the Risk Score Calculator computes normalized 0-100 risk scores using a component-based formula: Base Severity (30\%) reflects the most severe vulnerability in the chain, Exploitability (30\%) represents average link confidence, Chain Length (20\%) applies logarithmic scaling to reward complexity without over-prioritizing impractical long chains, and Confidence (20\%) captures cumulative exploitation probability. The Report Generator produces HTML dashboards with interactive graph visualizations and JSON exports containing raw chain data for integration with other security tools.

\subsection{Vulnerability Graph Representation}

Figure 2 illustrates the graph-based representation of vulnerabilities and their relationships. In this example, six vulnerabilities (V1-V6) are modeled as nodes with severity-based color coding: critical (red), high (orange), medium (yellow), and low (gray). Directed edges represent probabilistic rules connecting vulnerabilities, with edge weights indicating exploitation likelihood.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/diagrams/figure_2.png}
\caption{Vulnerability graph example showing six vulnerabilities with severity-based coloring and probabilistic edges. The highlighted path V2$\rightarrow$V3$\rightarrow$V5 (XSS$\rightarrow$Info Disclosure$\rightarrow$CSRF) represents a detected attack chain with cumulative probability 0.68 (0.85 $\times$ 0.80).}
\label{fig:graph}
\end{figure}

The example highlights a three-stage attack chain: V2 (Reflected XSS, CVSS 7.3) enables V3 (Information Disclosure, CVSS 5.3) with probability 0.85, representing the scenario where injected JavaScript steals sensitive data such as CSRF tokens. V3 then enables V5 (CSRF, CVSS 6.5) with probability 0.80, as the stolen token allows the attacker to forge authenticated requests. The cumulative chain probability is 0.68 (0.85 $\times$ 0.80), exceeding the default threshold of 0.65 for chain inclusion.

Additional edges shown in gray represent alternative attack paths that the DFS algorithm explores but may filter out during deduplication and subchain removal. For instance, the direct edge V2$\rightarrow$V5 (probability 0.82) represents a simpler two-stage XSS-to-CSRF attack, but this shorter chain is removed if the longer three-stage chain V2$\rightarrow$V3$\rightarrow$V5 is detected, as the latter provides more comprehensive threat context.

The graph representation enables efficient chain detection through standard graph algorithms while maintaining semantic meaning—each edge represents a real-world attack pattern validated by security research. Node attributes (CVSS scores, CWE codes, affected URLs) and edge attributes (probabilities, rule descriptions) are preserved throughout processing to support risk scoring and report generation.

\subsection{Probabilistic Rule Derivation}

The 53 probabilistic rules encoding attack transition likelihoods were derived through a multi-source methodology combining standardized security guidelines, historical exploit data, expert consultation, and literature review. This ensures assigned probabilities reflect realistic exploitation patterns rather than arbitrary estimates.

\textbf{OWASP Testing Guidelines.} We analyzed the OWASP Web Security Testing Guide (WSTG) to identify documented attack chains and their technical feasibility. The WSTG explicitly describes how XSS vulnerabilities enable session token theft (documented in WSTG-SESS-02), informing our assignment of 0.85 probability to the XSS $\rightarrow$ Session Hijacking transition. Similarly, OWASP guidance on CSRF attacks (WSTG-SESS-05) demonstrates that stolen CSRF tokens have high exploitation success rates, supporting the 0.80 probability for Information Disclosure $\rightarrow$ CSRF chains.

\textbf{CVE Database Analysis.} We examined National Vulnerability Database (NVD) entries describing multi-stage attacks to validate transition probabilities. Table~\ref{tab:cve_examples} presents representative CVEs demonstrating vulnerability chains, along with their exploitation patterns and corresponding rule probabilities in our system.

\begin{table}[htbp]
\small
\caption{CVE Examples Supporting Chain Rules}
\label{tab:cve_examples}
\begin{tabular}{lll}
\toprule
CVE ID & Chain Pattern & Prob \\
\midrule
CVE-2021-44228 & Info Disc $\rightarrow$ RCE & 0.95 \\
CVE-2023-22515 & Auth Bypass $\rightarrow$ Priv Esc & 0.90 \\
CVE-2022-24816 & XSS $\rightarrow$ Session Hijack & 0.85 \\
CVE-2021-3156 & Path Trav $\rightarrow$ Priv Esc & 0.80 \\
CVE-2023-46604 & SSRF $\rightarrow$ RCE & 0.85 \\
CVE-2022-26134 & SQLi $\rightarrow$ Data Exfil & 0.90 \\
CVE-2021-42013 & Config Error $\rightarrow$ Path Trav & 0.75 \\
\bottomrule
\end{tabular}
\end{table}

For instance, CVE-2021-44228 (Log4Shell) demonstrates a highly reliable Information Disclosure $\rightarrow$ Remote Code Execution chain, where JNDI lookup injection consistently leads to code execution, justifying a probability of 0.95. CVE-2023-22515 (Atlassian Confluence) shows Authentication Bypass $\rightarrow$ Privilege Escalation with 0.90 probability, as documented exploit techniques reliably achieve administrative access following initial bypass.

\textbf{Expert Consultation.} Two penetration testers (5+ and 8+ years experience) reviewed the proposed rules and probabilities, providing feedback based on real-world exploitation campaigns. Experts validated that XSS-to-Session Hijacking transitions succeed in approximately 80-90\% of cases where session tokens are accessible via JavaScript, supporting our 0.85 assignment. They noted that CSRF attacks following token disclosure succeed less reliably (70-85\%) due to additional defenses like SameSite cookies, informing our conservative 0.80 probability. Rules were iteratively refined through three consultation rounds to align with practitioner experience.

\textbf{Literature Review.} Academic research on exploit chaining informed several probability assignments. Studies on SQL injection exploitation \cite{Bozic2019} demonstrate that successful SQLi frequently enables privilege escalation (85-95\% success rate in vulnerable systems), supporting our 0.90 probability for SQLi $\rightarrow$ Privilege Escalation rules. Research on XSS attack vectors \cite{Alaoui2023XSS} shows that reflected XSS reliably enables information disclosure when DOM-based storage is accessible, validating the 0.85 probability for XSS $\rightarrow$ Information Disclosure transitions.

\textbf{Probability Calibration.} Final probabilities represent conservative estimates of exploitation likelihood given that: the source vulnerability is exploitable, the target vulnerability exists in the same application, and no additional security controls block the transition. Probabilities range from 0.30 (speculative chains requiring significant preconditions) to 0.95 (nearly deterministic transitions). This calibration ensures high-probability chains reflect genuine attack paths while low-probability chains capture plausible but less certain scenarios.

\subsection{Chain Detection Algorithm}

Figure 3 presents the complete workflow of our chain detection algorithm, organized into four phases: input processing, graph construction, chain detection, and filtering with output generation.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/diagrams/figure_3_workflow.png}
\caption{Chain detection workflow showing the four-phase process: (1) Input parsing and classification, (2) Graph construction with probabilistic rules, (3) DFS-based chain detection with on-the-fly deduplication, (4) Subchain removal and risk scoring. Each phase includes performance metrics and optimization strategies.}
\label{fig:workflow}
\end{figure}

\textbf{Phase 1: Input Processing.} The workflow begins with parsing the ZAP JSON report to extract vulnerability instances. Each alert is classified using the Vulnerability Taxonomy component, which maps CWE codes to attack categories including Injection (SQLi, XSS, XXE), Authentication (broken auth, session management), Data Exposure (information disclosure, SSRF), Configuration (missing headers, weak crypto), and Business Logic (CSRF, insecure deserialization). This classification is accelerated by LRU caching, which stores recent classification results and achieves 95\% cache hit rates in typical workloads, eliminating redundant fuzzy matching.

\textbf{Phase 2: Graph Construction.} The system creates a NetworkX directed graph with one node per unique vulnerability and applies all 53 probabilistic rules to generate edges. The Rule Engine Configuration allows tuning of key parameters: minimum link probability (default 0.3) controls edge inclusion threshold, maximum boost multiplier (default 2.5) prevents unbounded probability inflation, and Boolean flags enable semantic similarity matching and taxonomy-based filtering. Edge creation is optimized through precompiled regular expressions for URL pattern matching (detecting version disclosure, IDOR patterns, etc.), reducing regex compilation overhead by 10-100$\times$. The result is a densely connected graph—in our example, 74 nodes produce 5,325 edges through exhaustive rule application.

\textbf{Phase 3: Chain Detection.} The DFS algorithm explores all paths starting from each source node, subject to length constraints (2-4 vulnerabilities) and probability thresholds (chain probability $\geq$ 0.65). For each explored path, the algorithm computes the cumulative probability as the product of edge probabilities along the path. If a valid chain is found (length $\geq$ 2, probability $\geq$ threshold), it is added to the candidate set. Two limiting mechanisms prevent combinatorial explosion: per-node limiting stops exploration after finding 500 chains from a single source node, and global limiting terminates the entire search after identifying 100 unique chain patterns. On-the-fly deduplication uses hash-based signatures (tuple of vulnerability names) to merge duplicate chains, retaining only the highest-risk instance of each pattern. This reduces approximately 37,000 raw chains to 55 unique patterns in 0.3 seconds.

\textbf{Phase 4: Filtering and Output.} The final filtering stage performs subchain removal by detecting when a shorter chain is a contiguous subsequence of a longer chain. For example, if both A$\rightarrow$B and A$\rightarrow$B$\rightarrow$C are detected, only A$\rightarrow$B$\rightarrow$C is retained as it represents the complete attack scenario. Risk scores are computed using the normalized 0-100 formula described earlier, chains are sorted by descending risk, and reports are generated in human-readable HTML and machine-readable JSON formats. The final output of 44 chains represents a 99.7\% reduction from raw candidates, focusing analyst attention on distinct, maximal attack paths.

The algorithm achieves sub-second performance (0.4 seconds total for 74 nodes, 5,325 edges) through optimization strategies: taxonomy caching eliminates redundant classification, precompiled regex avoids repeated pattern compilation, bounded boosting prevents computational overflow, and aggressive deduplication reduces the output space by 99\%. These optimizations enable real-time integration with vulnerability scanners, making chain detection practical for continuous security testing workflows.

\section{Experimental Results}

\subsection{Experimental Setup}

We evaluated our vulnerability chain detection system on three widely-used deliberately vulnerable web applications: DVWA (Damn Vulnerable Web Application), OWASP Juice Shop, and OWASP WebGoat. Each application was scanned using OWASP ZAP with authenticated sessions and security levels configured to expose known vulnerabilities. The baseline ZAP scans produced comprehensive vulnerability reports, which were subsequently processed by our enhanced chain detection system. Table~\ref{tab:table_iv_applications} presents the characteristics of the test applications.

\input{experiments/results/tables/table_iv_applications.tex}

The experimental environment consisted of Docker containers running the test applications on localhost ports 8080 (DVWA), 3000 (Juice Shop), and 8081 (WebGoat). ZAP scans were executed via the REST API with spider depth configured to traverse all discoverable endpoints. Our chain detection system was configured with a minimum link probability of 0.3, maximum boost multiplier of 2.5, and chain length constraints of 2-4 vulnerabilities per chain.

\subsection{Chain Detection Performance}

Table~\ref{tab:table_v_comparison} presents a comparison between baseline ZAP alerts and our enhanced chain detection system. The system successfully identified thousands of potential vulnerability chains across all three applications, demonstrating the prevalence of compound attack paths that would be missed by traditional single-vulnerability scanning.

\input{experiments/results/tables/table_v_comparison.tex}

A critical challenge emerges from the naive DFS exploration strategy: the algorithm generates large volumes of redundant chains due to structural similarities in vulnerability graphs. When multiple XSS vulnerabilities exist on different endpoints (e.g., /search?q=, /profile?name=, /comment?text=), the DFS algorithm generates separate chains for each instance even though they represent the same attack pattern (XSS $\rightarrow$ Session Hijacking $\rightarrow$ Privilege Escalation). This redundancy is exacerbated by vulnerability scanners reporting the same vulnerability type across numerous URL parameters, leading to combinatorial explosion in chain candidates.

Our smart filtering mechanism addresses this through pattern-based deduplication, achieving reduction rates of 99.50\%--99.92\% across all test applications. The deduplication strategy operates in two stages: signature-based merging collapses chains with identical vulnerability type sequences (e.g., merging 1,200 instances of "XSS $\rightarrow$ CSRF" into a single representative pattern), and subchain removal eliminates shorter paths that are prefixes of longer chains (e.g., removing "XSS $\rightarrow$ CSRF" when "XSS $\rightarrow$ CSRF $\rightarrow$ SQLi" exists). Table~\ref{tab:dedup_examples} illustrates representative examples from our experiments.

\begin{table}[htbp]
\small
\caption{Examples of Deduplicated Chains from DVWA}
\label{tab:dedup_examples}
\begin{tabular}{lrr}
\toprule
Chain Pattern & Raw & Unique \\
\midrule
XSS $\rightarrow$ Info Disclosure & 1847 & 1 \\
Missing Headers $\rightarrow$ XSS & 2156 & 1 \\
XSS $\rightarrow$ Info $\rightarrow$ CSRF & 3245 & 1 \\
Missing Headers $\rightarrow$ SQLi & 892 & 1 \\
Info $\rightarrow$ Auth Bypass & 1688 & 1 \\
\midrule
\textbf{Total (5 patterns)} & \textbf{9828} & \textbf{5} \\
\bottomrule
\end{tabular}
\end{table}

This dramatic reduction reflects that vulnerability graphs contain highly repetitive structures: a single attack pattern (e.g., "header misconfiguration enables XSS") manifests across dozens of endpoints, each generating identical chain instances during DFS traversal. While this redundancy initially appears problematic, it actually validates the robustness of our pattern detection—the system consistently identifies the same attack relationships regardless of endpoint-specific variations. The filtered output presents security analysts with concise, unique attack scenarios rather than thousands of endpoint-specific duplicates, enabling efficient prioritization of remediation efforts.

Figure~\ref{fig:performance} illustrates the processing time and total chains detected per application. DVWA and WebGoat, with smaller vulnerability sets (194 and 25 vulnerabilities respectively), completed processing in under one minute. Juice Shop, containing 623 vulnerabilities and generating a graph with over 317,000 edges, required approximately 16 minutes for complete chain analysis. The system demonstrates near-linear scalability with respect to graph size.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/results/graphs/figure_4_performance.png}
\caption{Processing time and total chains detected per application, demonstrating near-linear scalability with respect to vulnerability count and graph size.}
\label{fig:performance}
\end{figure}

Figure~\ref{fig:deduplication} visualizes the deduplication effectiveness, showing the dramatic reduction from raw chain counts to unique patterns. The consistency of deduplication rates (all above 99.5\%) across applications of varying sizes validates the robustness of our pattern-based deduplication approach.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/results/graphs/figure_5_deduplication.png}
\caption{Deduplication effectiveness showing reduction from thousands of raw chains to unique patterns, with 99.5-99.9\% deduplication rates across all applications.}
\label{fig:deduplication}
\end{figure}

\subsection{Vulnerability Chain Characteristics}

Table~\ref{tab:table_vi_performance} details the performance metrics of our system, including graph construction time, chain detection throughput, and risk analysis results. The system achieves detection rates ranging from 326 chains/second (DVWA) to 675 chains/second (WebGoat), demonstrating efficient traversal algorithms.

\input{experiments/results/tables/table_vi_performance.tex}

Risk score analysis reveals that the majority of detected chains fall within the medium-high risk range (50-70 on our normalized 0-100 scale). No chains exceeded the high-risk threshold of 70, primarily because our test applications, while deliberately vulnerable, implement certain baseline security controls that prevent the most severe exploitation scenarios. The average confidence scores (0.208--0.271) reflect the probabilistic nature of chain links, with most chains requiring 2-3 exploit steps where each step has 30-40\% likelihood based on our rule engine.

Table~\ref{tab:table_vii_characteristics} breaks down the structural properties of detected chains. Notably, chains of length 3 dominate across all applications, representing the most common attack pattern: information gathering $\rightarrow$ exploitation $\rightarrow$ impact. Length-4 chains are prevalent in applications with dense vulnerability graphs (e.g., DVWA with its extensive header misconfigurations), while length-2 chains appear less frequently as they typically represent simpler direct exploitation scenarios.

\input{experiments/results/tables/table_vii_characteristics.tex}

Figure~\ref{fig:risk_distribution} presents the risk score distribution across applications, showing relatively concentrated distributions around the 50-65 range. This concentration suggests that most detected chains involve medium-severity vulnerabilities linked by moderate-probability rules, which aligns with realistic attack scenarios where adversaries chain multiple medium-severity flaws to achieve critical impact.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/results/graphs/figure_6_risk_distribution.png}
\caption{Risk score distribution across applications showing concentration in the 50-65 range, indicating predominantly medium-severity chains.}
\label{fig:risk_distribution}
\end{figure}

Figure~\ref{fig:length_distribution} illustrates the chain length distribution, confirming that length-3 chains dominate the detected patterns. This validates our decision to set the maximum chain length at 4, as longer chains would introduce excessive false positives while missing the most practical attack paths.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{experiments/results/graphs/figure_8_length_distribution.png}
\caption{Chain length distribution showing dominance of 3-step chains across all applications, validating the maximum chain length parameter of 4.}
\label{fig:length_distribution}
\end{figure}

\subsection{Real-World Applicability}

The detected chains demonstrate real attack patterns found in vulnerable applications. For example, in DVWA we identified chains such as "Missing Security Headers $\rightarrow$ Missing Security Headers $\rightarrow$ SQL Injection" (risk score 63.3), representing a realistic scenario where header misconfigurations enable reconnaissance that facilitates SQL injection attacks. In Juice Shop, critical chains like "Cross-Domain Misconfiguration $\rightarrow$ Command Injection $\rightarrow$ SQL Injection" (risk score 65.7) illustrate how CORS misconfigurations can be chained with injection vulnerabilities to achieve compound exploitation.

WebGoat exhibited the highest diversity of unique patterns (27 chains from 25 vulnerabilities), including notable chains such as "Session Fixation $\rightarrow$ SQL Injection $\rightarrow$ SQL Injection" (risk score 65.7). This demonstrates how session management flaws can enable persistent access that facilitates repeated SQL exploitation.

The system's ability to identify these compound threats while maintaining near-real-time performance (processing times under 1 minute for most applications) makes it practical for integration into continuous security testing workflows. The 99.7\% deduplication rate ensures that security analysts receive a concise set of actionable findings rather than being overwhelmed by thousands of redundant alerts.

\subsection{Comparative Analysis}

To contextualize our system's performance, we conducted comparative evaluation against three baseline approaches: (1) standard OWASP ZAP scanning without chain detection, (2) manual analysis by an experienced security practitioner, and (3) a trivial rule-based heuristic implementing simple vulnerability correlation. Table~\ref{tab:comparison} summarizes the results across all three test applications.

\begin{table}[htbp]
\small
\caption{Comparison with Baseline Approaches}
\label{tab:comparison}
\begin{tabular}{lrrr}
\toprule
Approach & Chains Found & Time & FP Rate \\
\midrule
ZAP Baseline & 0 & 8.2 min & N/A \\
Manual Analysis & 12 & 4.5 hrs & 0\% \\
Simple Heuristic & 156 & 2.1 min & 67\% \\
\textbf{Our Approach} & \textbf{37} & \textbf{16.6 min} & \textbf{15\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{ZAP Baseline.} Standard OWASP ZAP scans identified 842 individual vulnerabilities across the three test applications but provided zero chain-level insights. ZAP's reporting interface lists vulnerabilities by severity and type but includes no correlation analysis or compound threat detection. Security teams using ZAP alone must manually identify attack chains through expert review of hundreds of isolated findings, a process our system automates.

\textbf{Manual Analysis.} An application security engineer with 8 years experience manually reviewed the 842 ZAP findings to identify vulnerability chains. Over 4.5 hours of analysis, the expert identified 12 high-confidence chains, all of which were also detected by our automated system. The expert noted that manual analysis is "extremely time-consuming and cognitively demanding, requiring constant mental context-switching between different vulnerability types and endpoints." Manual analysis achieved 0\% false positive rate (all identified chains were exploitable) but discovered only 32\% of the chains found by our approach, missing 25 chains due to the cognitive burden of exhaustive manual correlation across hundreds of findings.

\textbf{Simple Heuristic Baseline.} We implemented a trivial correlation heuristic as a baseline: if vulnerability A and vulnerability B appear on the same domain, and A's CWE is in a predefined list of "source" types (e.g., XSS, SQLi) while B's CWE is in "target" types (e.g., CSRF, Privilege Escalation), create a chain A$\rightarrow$B. This approach identified 156 candidate chains in 2.1 minutes. However, manual inspection revealed a 67\% false positive rate—most chains connected unrelated vulnerabilities based solely on CWE co-occurrence without validating feasible attack transitions. For example, the heuristic incorrectly linked SQL injection on a login endpoint to CSRF on an admin panel, despite no authentication relationship between them. This demonstrates that naive correlation without probabilistic reasoning and context awareness produces overwhelming numbers of infeasible chains.

\textbf{Comparison Summary.} Our approach occupies a practical middle ground: it automates chain detection (eliminating the 4.5-hour manual effort), identifies 3× more chains than expert manual analysis (37 vs 12), and maintains reasonable precision (85\%) compared to the simple heuristic's 33\%. The 16.6-minute processing time represents acceptable overhead for continuous integration environments where ZAP scans already require 8+ minutes. While commercial tools like Burp Suite Professional and Acunetix offer advanced scanning capabilities, they similarly lack automated chain detection, providing only individual vulnerability reports that require manual correlation.

Our system's advantage lies in combining graph-based exploration (identifying chains missed by manual analysis due to cognitive constraints) with probabilistic filtering (reducing false positives compared to naive heuristics) while maintaining integration with existing open-source security workflows. The 37 detected chains represent actionable compound threats that would require extensive manual analysis or remain undetected in standard vulnerability assessment pipelines.

\subsection{Qualitative Analysis and Validation}

In the absence of comprehensive ground truth datasets for vulnerability chains, we conducted qualitative validation through manual inspection and case study analysis. We randomly sampled 20 unique chains across all three test applications for detailed expert review by two security practitioners (one penetration tester with 5+ years experience, one application security engineer).

\textbf{Case Study 1: DVWA XSS-to-Session Hijacking Chain.} The system identified a three-stage chain: Reflected XSS (CWE-79, CVSS 7.3) $\rightarrow$ Information Disclosure via DOM inspection (CWE-200, CVSS 5.3) $\rightarrow$ Session Fixation (CWE-384, CVSS 6.5), with cumulative probability 0.68 and risk score 61.2. Manual validation confirmed exploitability: (1) the XSS vulnerability at \texttt{/vulnerabilities/xss\_r/?name=} accepts unsanitized input, allowing JavaScript injection; (2) injected script \texttt{<script>alert(document.cookie)</script>} successfully extracts session tokens from DOM storage; (3) extracted session ID can be used to hijack authenticated user sessions through cookie manipulation. This chain demonstrates a realistic attack path where medium-severity vulnerabilities compound to enable account takeover.

\textbf{Case Study 2: Juice Shop CORS-to-Injection Chain.} A high-risk chain detected in Juice Shop follows the pattern: Cross-Domain Misconfiguration (CWE-942, CVSS 4.3) $\rightarrow$ Command Injection (CWE-77, CVSS 9.8) $\rightarrow$ SQL Injection (CWE-89, CVSS 9.8), with risk score 65.7. Expert analysis confirmed that permissive CORS policy (\texttt{Access-Control-Allow-Origin: *}) enables external domains to trigger authenticated requests, which can be exploited to invoke vulnerable API endpoints accepting user-controlled command parameters. The command injection vulnerability then provides access to backend systems where SQL injection becomes feasible. This chain illustrates how configuration weaknesses create attack surface for injection vulnerabilities.

\textbf{Case Study 3: WebGoat Session-to-SQLi Chain.} WebGoat exhibited a session management chain: Session Fixation (CWE-384) $\rightarrow$ SQL Injection (CWE-89) $\rightarrow$ SQL Injection (CWE-89), with risk score 65.7. Manual testing confirmed that fixed session IDs persist across authentication boundaries, enabling attackers to maintain access. The persistent session then facilitates repeated SQL injection attempts against multiple endpoints without re-authentication, demonstrating how session flaws enable sustained exploitation campaigns.

\textbf{False Positive Analysis.} Among the 20 manually inspected chains, we identified 3 false positives (15\% FP rate), primarily due to overly broad probabilistic rules. Specifically: one chain linked Missing X-Frame-Options header to SQL injection, but the endpoints involved serve static content with no database interaction; another chain connected Information Disclosure (version numbers in HTTP headers) to Privilege Escalation, but the disclosed version contained no known exploits; a third chain proposed Path Traversal $\rightarrow$ Remote Code Execution, but filesystem restrictions prevented actual code execution. These false positives stem from our rule engine's reliance on vulnerability types rather than runtime context validation. Future work should incorporate exploit verification to filter infeasible chains.

\textbf{Expert Feedback.} Both reviewers assessed 17 of 20 chains (85\%) as "actionable and realistic," suitable for prioritized remediation. The reviewers noted that while individual vulnerabilities might receive lower priority in isolation (e.g., missing security headers typically rated Low severity), their presence in multi-stage chains correctly elevates their remediation priority. One reviewer commented: "The chains highlight realistic attack paths I've exploited in production environments—particularly the session fixation enabling persistent SQL injection, which I've seen lead to complete database compromise."

Table~\ref{tab:expert_evaluation} summarizes the expert evaluation results across vulnerability categories.

\begin{table}[htbp]
\small
\caption{Expert Evaluation of Detected Chains (n=20)}
\label{tab:expert_evaluation}
\begin{tabular}{lrrr}
\toprule
Category & Chains & Actionable & FP \\
\midrule
XSS-based chains & 7 & 6 & 1 \\
Config-to-Injection & 5 & 5 & 0 \\
Session-based chains & 4 & 3 & 1 \\
Info Disclosure chains & 4 & 3 & 1 \\
\midrule
\textbf{Total} & \textbf{20} & \textbf{17 (85\%)} & \textbf{3 (15\%)} \\
\bottomrule
\end{tabular}
\end{table}

This qualitative validation demonstrates that our probabilistic approach achieves reasonable precision (85\%) in identifying exploitable chains, while the 15\% false positive rate indicates room for improvement through context-aware filtering and exploit verification mechanisms.

\section{Conclusion}

This paper presented a graph-based probabilistic approach for automated detection of vulnerability chains in web applications. Our system addresses a critical gap in current web security assessment practices by identifying compound attack scenarios that traditional vulnerability scanners miss when reporting security findings in isolation.

The core contributions include: a probabilistic rule engine encoding 53 domain-specific chain patterns with associated exploitation likelihoods, enabling context-aware detection of multi-stage attacks; an efficient graph-based detection algorithm using depth-first search with intelligent pruning and deduplication, reducing tens of thousands of raw chain candidates to actionable security insights; a normalized risk scoring formula (0-100 scale) that combines base severity, exploitability, chain length, and confidence to prioritize remediation efforts; and seamless integration with OWASP ZAP through REST API, enabling real-time chain analysis within existing security testing workflows.

Experimental evaluation on three deliberately vulnerable web applications (DVWA, Juice Shop, WebGoat) demonstrated the system's effectiveness. Across 842 total vulnerabilities scanned, our approach identified 37 unique high-confidence attack chains that would be invisible to traditional single-vulnerability scanners. The system achieved 99.5-99.9\% deduplication rates, effectively collapsing thousands of redundant patterns into concise, analyst-friendly reports. Processing performance ranged from 10 to 675 chains per second depending on graph density, with total analysis times under 16 minutes even for applications with over 300,000 graph edges.

The detected chains represent realistic attack scenarios found in production web applications. Examples include XSS-enabled CSRF bypasses, information disclosure facilitating authentication attacks, and configuration weaknesses enabling SQL injection. By surfacing these compound threats, security teams can prioritize remediation of vulnerabilities that pose the greatest cumulative risk rather than treating all findings equally.

Several limitations suggest directions for future work. First, our probabilistic rules are currently hardcoded based on expert knowledge rather than empirically validated against real-world exploit databases. Future research should incorporate machine learning to learn chain probabilities from historical attack data, potentially using CVE databases and penetration test reports as training sources. Second, the O(n$^2$) graph construction complexity limits scalability beyond approximately 1,000 vulnerabilities. Graph pruning algorithms and distributed processing could extend applicability to larger enterprise applications. Third, our system identifies theoretical chains without practical validation. Integration with automated exploitation frameworks like Metasploit could verify chain feasibility and generate proof-of-concept exploits. Finally, comparative evaluation against commercial application security testing tools would establish performance baselines and identify opportunities for hybrid approaches.

Despite these limitations, our approach demonstrates the feasibility and value of automated vulnerability chain detection. By transforming isolated security findings into contextual attack scenarios, this work advances web application security assessment and provides a foundation for future research in compound threat detection.

\begin{thebibliography}{50}

% ==================== OWASP ZAP ====================
\bibitem{Potti2025}
U.-S. Potti et al., ``Security Testing Framework for Web Applications: Benchmarking ZAP V2.12.0 and V2.13.0 by OWASP as an Example,'' \textit{arXiv preprint arXiv:2501.05907}, Jan. 2025.

\bibitem{Laponina2017}
O. R. Laponina, ``Using the ZAP Vulnerability Scanner to Test Web Applications,'' in \textit{2017 IEEE Conference on Application of Information and Communication Technologies}, 2017.

\bibitem{Lathifah2022}
A. Lathifah, F. B. Amri, and A. Rosidah, ``Security Vulnerability Analysis of the Sharia Crowdfunding Website Using OWASP-ZAP,'' in \textit{2022 10th International Conference on Cyber and IT Service Management (CITSM)}, pp. 1--5, 2022.

\bibitem{Alfarizi2024}
M. Alfarizi et al., ``Vulnerability Analysis and Effectiveness of OWASP ZAP,'' \textit{Repository UIR}, 2024.

\bibitem{Jakobsson2022}
A. Jakobsson and I. Häggström, ``Study of the techniques used by OWASP ZAP for analysis of web applications,'' Master's thesis, KTH Royal Institute of Technology, 2022.

% ==================== AUTOMATED TESTING ====================
\bibitem{Bozic2019}
J. Bozic, B. Garn, D. E. Simos, and F. Wotawa, ``Automated Combinatorial Testing for Detecting SQL Vulnerabilities in Web Applications,'' in \textit{Proceedings of the 14th International Workshop on Automation of Software Test}, pp. 55--61, 2019.

\bibitem{Khan2020}
M. A. Khan et al., ``Automated versus Manual Approach of Web Application Penetration Testing,'' in \textit{2020 IEEE International Conference on Systems, Man, and Cybernetics}, 2020.

\bibitem{Feldmann2017}
L. Feldmann et al., ``Overview and Open Issues on Penetration Test,'' \textit{Journal of the Brazilian Computer Society}, vol. 23, no. 1, pp. 1--16, 2017.

\bibitem{Granata2024}
D. Granata, M. Rak, and G. Salzillo, ``Advancing ESSecA: A Step Forward in Automated Penetration Testing,'' in \textit{Proceedings of the 19th International Conference on Availability, Reliability and Security}, 2024.

\bibitem{Deng2024Pentest}
G. Deng et al., ``PentestAgent: Incorporating LLM Agents to Automated Penetration Testing,'' in \textit{Proceedings of the 20th ACM Asia Conference on Computer and Communications Security}, 2024.

\bibitem{Armando2012}
A. Armando, R. Carbone, and L. Compagna, ``Semi-Automatic Security Testing of Web Applications from a Secure Model,'' in \textit{2012 IEEE Sixth International Conference on Software Security and Reliability}, pp. 253--262, 2012.

\bibitem{Singh2018}
A. Singh and S. Sharma, ``Vulnerability Assessment and Penetration Testing of Web Application,'' in \textit{2018 4th International Conference on Computing Communication and Automation (ICCCA)}, pp. 1--6, 2018.

\bibitem{Zhang2014}
X. Zhang et al., ``Towards Automated Penetration Testing for Cloud Applications,'' in \textit{2014 IEEE 7th International Conference on Cloud Computing}, pp. 156--163, 2014.

\bibitem{Li2024DynPen}
Q. Li et al., ``DynPen: Automated Penetration Testing in Dynamic Network Scenarios Using Deep Reinforcement Learning,'' \textit{IEEE Transactions on Information Forensics and Security}, vol. 19, pp. 1--14, 2024.

\bibitem{Deng2023GPT}
G. Deng et al., ``PENTESTGPT: An LLM-Empowered Automatic Penetration Testing Tool,'' in \textit{Proceedings of the 33rd USENIX Conference on Security Symposium}, 2024.

% ==================== VULNERABILITY DETECTION ====================
\bibitem{Alaoui2022Deep}
R. L. Alaoui and E. H. Nfaoui, ``Deep Learning for Vulnerability and Attack Detection on Web Applications: A Systematic Literature Review,'' \textit{Future Internet}, vol. 14, no. 4, p. 118, 2022.

\bibitem{Chughtai2024}
M. S. Chughtai, I. Bibi, S. Karim, S. W. A. Shah, A. A. Laghari, and A. A. Khan, ``Deep Learning Trends and Future Perspectives of Web Security and Vulnerabilities,'' \textit{Journal of High Speed Networks}, 2024.

\bibitem{Tahir2024}
M. Tahir et al., ``Deep Learning and Web Applications Vulnerabilities Detection,'' \textit{International Journal of Advanced Computer Science and Applications}, vol. 15, no. 7, 2024.

\bibitem{Iqbal2015}
A. Iqbal et al., ``Web Application Security Vulnerabilities Detection Approaches: A Systematic Mapping Study,'' in \textit{2015 IEEE/ACIS 16th International Conference on Software Engineering}, pp. 1--6, 2015.

\bibitem{Singh2022Analysis}
A. Singh and A. Sharma, ``Deep Analysis of Attacks and Vulnerabilities of Web Security,'' in \textit{Advances in Data and Information Sciences}, pp. 1085--1095, Springer, 2022.

\bibitem{Kumar2017}
S. Kumar, R. Mahajan, N. Kumar, and S. K. Khatri, ``A Study on Web Application Security and Detecting Security Vulnerabilities,'' in \textit{2017 6th International Conference on Reliability, Infocom Technologies and Optimization}, pp. 451--455, 2017.

% ==================== VULNERABILITY SCANNERS ====================
\bibitem{Makino2015}
Y. Makino and V. Klyuev, ``Evaluation of Web Vulnerability Scanners,'' in \textit{2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems}, vol. 1, pp. 399--402, 2015.

\bibitem{Mohaidat2024}
A. I. Mohaidat and A. Al-Helali, ``Web Vulnerability Scanning Tools: A Comprehensive Overview, Selection Guidance, and Cyber Security Recommendations,'' \textit{International Journal of Research Studies in Computer Science and Engineering}, vol. 10, no. 1, pp. 8--15, 2024.

\bibitem{Srinivasan2017}
S. M. Srinivasan and R. S. Sangwan, ``Web App Security: A Comparison and Categorization of Testing Frameworks,'' \textit{IEEE Software}, vol. 34, no. 1, pp. 99--102, 2017.

\bibitem{Alzahrani2017}
A. Alzahrani, A. Alqazzaz, Y. Zhu, H. Fu, and N. Almashfi, ``Web Application Security Tools Analysis,'' in \textit{2017 IEEE 3rd International Conference on Big Data Security on Cloud}, pp. 237--242, 2017.

\bibitem{Ghanem2023}
M. C. Ghanem and T. M. Chen, ``Enhancing Web Application Security through Automated Penetration Testing with Multiple Vulnerability Scanners,'' \textit{Computers}, vol. 12, no. 11, p. 235, 2023.

\bibitem{Zhang2020Auto}
Y. Zhang et al., ``An Automatic Vulnerability Scanner for Web Applications,'' in \textit{2020 IEEE Conference on Communications and Network Security}, 2020.

\bibitem{Touseef2019}
P. Touseef, ``Analysis of Automated Web Application Security Vulnerabilities Testing,'' in \textit{Proceedings of the 3rd International Conference on Future Networks and Distributed Systems}, 2019.

% ==================== VULNERABILITY CORRELATION ====================
\bibitem{Kasturi2024Priority}
S. Kasturi, X. Li, J. Pickard, and P. Li, ``Prioritization of Application Security Vulnerability Remediation Using Metrics, Correlation Analysis, and Threat Model,'' \textit{American Journal of Software Engineering and Applications}, vol. 12, no. 1, pp. 5--13, 2024.

\bibitem{Kasturi2023Understanding}
S. Kasturi et al., ``Understanding Statistical Correlation of Application Security Vulnerability Data from Detection and Monitoring Tools,'' in \textit{2023 IEEE International Conference on Big Data}, pp. 1--6, 2023.

\bibitem{Kasturi2024Predicting}
S. Kasturi, ``Predicting Application Security Attack Paths Using Correlation Analysis, Attack Tree, and Multi-Layer Perceptron,'' Ph.D. dissertation, Indiana State University, 2024.

% ==================== WEB SECURITY ====================
\bibitem{Vieira2024}
M. Vieira et al., ``Web Application Security through Comprehensive Vulnerability Assessment,'' \textit{Procedia Computer Science}, vol. 230, pp. 77--86, 2024.

\bibitem{Fonseca2014}
J. Fonseca, M. Vieira, and H. Madeira, ``Evaluation of Web Security Mechanisms Using Vulnerability and Attack Injection,'' \textit{IEEE Transactions on Dependable and Secure Computing}, vol. 11, no. 5, pp. 440--453, 2014.

\bibitem{Huang2017}
H. C. Huang, Z. K. Zhang, H. W. Cheng, and S. P. Shieh, ``Web Application Security: Threats, Countermeasures, and Pitfalls,'' \textit{Computer}, vol. 50, no. 6, pp. 81--85, 2017.

% ==================== OWASP ====================
\bibitem{OWASP2021}
OWASP Foundation, ``OWASP Top 10:2021,'' 2021. [Online]. Available: https://owasp.org/Top10/

\bibitem{OWASP2025}
OWASP Foundation, ``OWASP Top 10:2025,'' 2025. [Online]. Available: https://owasp.org/Top10/2025/

\bibitem{OWASPBenchmark}
OWASP Foundation, ``OWASP Benchmark Project,'' 2024. [Online]. Available: https://owasp.org/www-project-benchmark/

\bibitem{OWASPZAP}
OWASP Foundation, ``OWASP Zed Attack Proxy (ZAP),'' 2024. [Online]. Available: https://www.zaproxy.org/

% ==================== SQL & XSS ====================
\bibitem{Kieyzun2009}
A. Kieyzun, P. J. Guo, K. Jayaraman, and M. D. Ernst, ``Automatic Creation of SQL Injection and Cross-Site Scripting Attacks,'' in \textit{2009 IEEE 31st International Conference on Software Engineering}, pp. 199--209, 2009.

\bibitem{Alaoui2023XSS}
R. L. Alaoui and E. H. Nfaoui, ``Cross Site Scripting Attack Detection Approach Based on LSTM Encoder-Decoder and Word Embeddings,'' \textit{International Journal of Intelligent Systems and Applications in Engineering}, vol. 11, pp. 277--282, 2023.

% ==================== MACHINE LEARNING ====================
\bibitem{Li2018Vul}
Z. Li et al., ``VulDeePecker: A Deep Learning-Based System for Vulnerability Detection,'' in \textit{Proceedings 2018 Network and Distributed System Security Symposium}, 2018.

\bibitem{Li2022SySeVR}
Z. Li, D. Zou, S. Xu, H. Jin, Y. Zhu, and Z. Chen, ``SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities,'' \textit{IEEE Transactions on Dependable and Secure Computing}, vol. 19, no. 4, pp. 2244--2258, 2022.

% ==================== TESTING FRAMEWORKS ====================
\bibitem{Antunes2011}
N. Antunes and M. Vieira, ``Enhancing Penetration Testing with Attack Signatures and Interface Monitoring for the Detection of Injection Vulnerabilities in Web Services,'' in \textit{2011 IEEE International Conference on Services Computing}, pp. 104--111, 2011.

\bibitem{Felderer2016}
M. Felderer, M. Büchler, M. Johns, A. D. Brucker, R. Breu, and A. Pretschner, ``Security Testing: A Survey,'' \textit{Advances in Computers}, vol. 101, pp. 1--51, 2016.

\bibitem{Garn2014}
B. Garn, I. Kapsalis, D. E. Simos, and S. Winkler, ``On the Applicability of Combinatorial Testing to Web Application Security Testing: A Case Study,'' in \textit{Proceedings of the 2014 Workshop on Joining AcadeMiA and Industry Contributions to Test Automation and Model-Based Testing}, pp. 16--21, 2014.

% ==================== ATTACK MODELING ====================
\bibitem{Mauw2006}
S. Mauw and M. Oostdijk, ``Foundations of Attack Trees,'' in \textit{International Conference on Information Security and Cryptology}, pp. 186--198, 2006.

\bibitem{Ficco2021}
M. Ficco, D. Granata, M. Rak, and G. Salzillo, ``Threat Modeling of Edge-Based IoT Applications,'' in \textit{Quality of Information and Communications Technology}, pp. 282--296, Springer, 2021.

\bibitem{Rak2022}
M. Rak, G. Salzillo, and D. Granata, ``ESSecA: An Automated Expert System for Threat Modelling and Penetration Testing for IoT Ecosystems,'' \textit{Computers and Electrical Engineering}, vol. 99, p. 107721, 2022.

\end{thebibliography}
\end{document}