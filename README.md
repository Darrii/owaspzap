# Vulnerability Chain Detection System

**Graph-based detection of compound exploits in web applications**

[![Python](https://img.shields.io/badge/python-3.11+-blue)]() [![License](https://img.shields.io/badge/license-MIT-green)]()

## What is this?

Traditional scanners find isolated vulnerabilities. This system **connects the dots** to discover attack chains:

```
XSS ‚Üí Steal Cookie ‚Üí Session Hijacking ‚Üí Admin Takeover
```

**Status**: Research prototype for Q2 academic publication. **NOT production-ready** ‚Äî see [Limitations](#limitations-and-future-work).

## Quick Start

```bash
# 1. Install
pip install -r requirements.txt

# 2. Start OWASP ZAP
docker run -p 8090:8090 -e ZAP_API_KEY=changeme zaproxy/zap-stable zap.sh -daemon \
  -host 0.0.0.0 -port 8090 -config api.key=changeme

# 3. Launch Web UI
python3 web_ui_app.py
# ‚Üí Open http://localhost:8888
```

## Key Features

- üîó **Chain Detection**: Find A‚ÜíB‚ÜíC attack paths (graph-based DFS)
- üìä **53 Expert Rules**: OWASP-based probabilistic patterns
- ‚ö° **Performance**: 0.4s for 74 vulnerabilities (1291√ó cached taxonomy)
- üîß **Configurable**: Tune thresholds, boost caps, probability filters
- üìà **Interactive Reports**: HTML + JSON export with risk scoring

## Example Output

**Input**: 129 ZAP alerts
**Processing**: Graph analysis (74 nodes, 5,325 edges)
**Output**: 44 validated chains

**Top chain**: `XSS ‚Üí Session Hijacking ‚Üí Privilege Escalation` (Risk: 87/100, Confidence: 0.68)

---

## üìñ Full Documentation

> **For humans**: Read sections above for quick overview
> **For AI/Claude**: Read detailed sections below for complete context

---

### Project Context

**Current Status**: Research prototype developed for Q2 academic journal publication.

**NOT a production security tool** - this is a proof-of-concept demonstrating the feasibility of automated vulnerability chain detection using graph-based analysis.

**Key Characteristics**:
- Graph-based approach (NetworkX directed graph)
- Probabilistic rule engine (56 rules with expert-assigned probabilities)
- **Configurable parameters** for threshold tuning (min_probability, max_boost)
- **Bounded boosting** to ensure mathematical soundness (cap at 2.5√ó)
- **Precompiled regex** for 10-100√ó faster URL parsing
- DFS-based chain discovery with smart filtering
- FastAPI web interface for demonstration purposes
- Integration with OWASP ZAP as vulnerability scanner

**Known Limitations** (see "Limitations and Future Work" section):
- Probabilities are hardcoded, not empirically validated
- Risk scoring formula uses arbitrary weights
- Graph scalability limited to ~200 vulnerabilities
- No validation that chains actually work in practice
- Deduplication ignores URL context

### For New Contributors / AI Assistants

**If you're continuing work on this project**, here's what you MUST understand:

1. **This is a research prototype**, not production code
   - Focus on demonstrating feasibility, not robustness
   - Many "optimizations" are actually limitations (e.g., truncating results)
   - Code quality is academic-level, not enterprise-level

2. **The graph construction is O(n¬≤)** and will break at scale
   - 74 vulnerabilities ‚Üí 5,325 edges is manageable
   - 200 vulnerabilities ‚Üí ~40,000 edges starts slowing down
   - 1000+ vulnerabilities ‚Üí system will hang
   - DO NOT try to "fix" this without fundamental redesign

3. **Probabilities are fake** (expert-assigned, not data-driven)
   - p=0.85 for "XSS‚ÜíCSRF" is a guess based on experience
   - They don't adapt to application context
   - Changing them requires understanding attack patterns

4. **Risk scoring formula is arbitrary**
   ```python
   risk = (length √ó 15) + (max_cvss √ó 7) + (probability √ó 10)
   ```
   - The numbers 15, 7, 10 have NO scientific basis
   - Longer chains get higher scores (counter-intuitive but intentional for academic paper)
   - DO NOT change without discussing rationale

5. **Key files and their purposes**:
   - `web_ui_app.py` - FastAPI backend, contains smart filtering logic
   - `vulnerability_chains/core/enhanced_detector.py` - Graph builder + DFS detector
   - `vulnerability_chains/rules/probabilistic_rules.py` - 56 probabilistic rules with configurable engine
   - `vulnerability_chains/utils/zap_parser.py` - ZAP JSON parser with deduplication
   - `vulnerability_chains/models.py` - Data structures (Vulnerability, Chain)

6. **Common misconceptions**:
   - "0.4 seconds" refers to graph analysis ONLY, not total scan time (8 hours)
   - "44 chains" is AFTER aggressive filtering (started with 37,000)
   - Subchain removal HIDES shorter chains (intentional, but controversial)
   - System finds theoretical chains, NOT validated exploits

7. **What NOT to do**:
   - Don't add ML without understanding current limitations
   - Don't "optimize" graph construction without benchmarking
   - Don't change risk formula without documenting rationale
   - Don't remove limitations section from README

8. **What TO do**:
   - Read "Limitations and Future Work" section carefully
   - Understand graph construction before modifying
   - Add tests if changing core algorithm
   - Document assumptions and trade-offs

## Features

- **Real-time ZAP Integration**: Direct connection to OWASP ZAP API for live scanning
- **Probabilistic Chain Detection**: 56 chain rules with confidence scoring
- **Configurable Rule Engine**: Tune thresholds (min_probability=0.3, max_boost=2.5) for precision-recall balance
- **Bounded Boosting**: Mathematical soundness with capped cumulative multipliers
- **Smart Deduplication**: Filters duplicate chains and subchains automatically
- **Web-based UI**: Modern interface for scan management and chain analysis
- **Interactive Reports**: HTML/JSON export with detailed vulnerability chains and metadata
- **Performance Optimized**: Precompiled regex patterns (10-100√ó speedup), handles 100+ vulnerabilities with sub-second analysis

## Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start OWASP ZAP
```bash
# Docker
docker run -p 8090:8090 -e ZAP_API_KEY=changeme zaproxy/zap-stable zap.sh -daemon \
  -host 0.0.0.0 -port 8090 -config api.key=changeme -config api.addrs.addr.name=.* \
  -config api.addrs.addr.regex=true

# Or use ZAP Desktop with API enabled
```

### 3. Launch Web UI
```bash
python3 web_ui_app.py
# Open http://localhost:8888
```

### 4. Scan and Analyze
1. Enter target URL
2. Start ZAP scan
3. Analyze vulnerability chains
4. Review detected chains with risk scores

## Architecture

### System Overview

The system follows a **pipeline architecture** with 8 distinct stages:

```
Web App ‚Üí ZAP Scanner ‚Üí Parser ‚Üí Graph Builder ‚Üí Rule Engine ‚Üí
DFS Detector ‚Üí Smart Filter ‚Üí Report Generator
```

### Core Components

```
vulnerability_chains/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_detector.py    # Graph-based chain detection
‚îÇ   ‚îú‚îÄ‚îÄ chain_detector.py        # DFS pathfinding algorithm
‚îÇ   ‚îú‚îÄ‚îÄ taxonomy.py              # Vulnerability classification
‚îÇ   ‚îî‚îÄ‚îÄ context_analyzer.py      # Contextual clustering
‚îú‚îÄ‚îÄ rules/
‚îÇ   ‚îî‚îÄ‚îÄ probabilistic_rules.py   # 24 chain rules with probabilities
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ zap_parser.py            # ZAP JSON report parser
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ chain_rules.json         # Chain rule definitions
‚îî‚îÄ‚îÄ visualization/
    ‚îî‚îÄ‚îÄ graph_visualizer.py      # HTML report generation

web_ui_app.py                    # FastAPI backend
web_ui/
‚îî‚îÄ‚îÄ index.html                   # Frontend interface
```

### Component Details

#### 1. OWASP ZAP Scanner
**Purpose**: Active vulnerability scanning of web applications

**Configuration**:
- Port: 8090
- API Key: "changeme"
- Scan Types: Spider (crawl) + Active Scan (inject payloads)
- Max Depth: 5 levels
- Timeout: 28800s (8 hours)

**Output**: JSON report with vulnerability alerts

**Integration**:
```python
# In web_ui_app.py
zap_client = ZAPClient(api_url="http://localhost:8090", api_key="changeme")
scan_results = await zap_client.active_scan(target_url, depth=5)
```

#### 2. ZAP Alert Parser (`zap_parser.py`)
**Purpose**: Convert ZAP JSON to internal vulnerability objects

**Process**:
1. Extract alerts from JSON
2. Parse metadata (name, severity, URL, CWE)
3. Deduplicate by signature (`alert_name|url_base`)
4. Map to Vulnerability dataclass

**Key Logic**:
```python
signature = f"{alert_name}|{url_base}"
if signature not in seen_alerts:
    vulnerabilities.append(Vulnerability(...))
```

**Input**: 129 raw alerts ‚Üí **Output**: 74 unique vulnerabilities

#### 3. Vulnerability Taxonomy (`taxonomy.py`)
**Purpose**: Classify vulnerabilities into categories

**Categories**:
- Injection (SQLi, XSS, XXE, Command Injection)
- Authentication (Broken Auth, Session Management)
- Data Exposure (Info Disclosure, SSRF, Path Traversal)
- Configuration (Missing Headers, Weak Crypto)
- Business Logic (CSRF, Insecure Deserialization)

**Mapping**: CWE code ‚Üí Category
```python
def categorize(vuln):
    if vuln.cwe in [79, 80, 85]:
        return "Cross-Site Scripting"
    elif vuln.cwe in [89, 564]:
        return "SQL Injection"
    # ... 24 total mappings
```

#### 4. Graph Builder (`enhanced_detector.py`)
**Purpose**: Build NetworkX directed graph from vulnerabilities

**Process**:
1. Create nodes (one per vulnerability)
2. Generate edges using 24 probabilistic rules
3. Optionally add cluster links (disabled for performance)

**Graph Structure**:
- **Nodes**: 74 vulnerability objects
- **Edges**: 5,325 directed edges with probability weights
- **Attributes**: Each edge has `probability` (0.0-1.0)

**Code**:
```python
graph = nx.DiGraph()
for vuln in vulnerabilities:
    graph.add_node(vuln.id, data=vuln)

for rule in rule_engine.rules:
    if rule.matches(vuln1, vuln2):
        graph.add_edge(vuln1.id, vuln2.id, probability=rule.probability)
```

#### 5. Probabilistic Rule Engine (`probabilistic_rules.py`)
**Purpose**: Define and apply 24 chain rules

**Rule Structure**:
```json
{
  "from_category": "Cross-Site Scripting",
  "to_category": "Information Disclosure",
  "probability": 0.85,
  "description": "XSS can steal sensitive data via JavaScript"
}
```

**Rule Application**:
- Check if vuln1.category matches `from_category`
- Check if vuln2.category matches `to_category`
- If match ‚Üí add edge with `probability`

**Domain Knowledge**: Probabilities assigned by security experts based on:
- OWASP Top 10 attack patterns
- CVE database analysis
- Penetration testing experience

#### 6. DFS Chain Detector (`enhanced_detector.py`)
**Purpose**: Find all vulnerability chains using depth-first search

**Algorithm**:
```python
def _dfs_chains(current, path, visited, chains):
    # Base case: save chain if length 2-4
    if 2 <= len(path) <= 4:
        chains.append(path)

    # Recursive case: explore neighbors
    for neighbor in graph.neighbors(current):
        if neighbor not in visited:
            _dfs_chains(neighbor, path + [neighbor], visited | {neighbor}, chains)
```

**Parameters**:
- min_length: 2 vulnerabilities
- max_length: 4 vulnerabilities
- min_chain_probability: 0.65 (product of edge probabilities)
- max_chains_per_node: 500 (prevent explosion)
- max_unique_patterns: 100 (global limit)

**Performance**:
- Explores ~37,000 raw paths
- On-the-fly deduplication reduces to 55 unique
- Execution time: 0.3 seconds

#### 7. Smart Filter (`web_ui_app.py`)
**Purpose**: Remove duplicate and subchains

**Stage 1 - Deduplication**:
```python
signature = tuple(v.name for v in chain)
if signature not in seen:
    unique_chains[signature] = chain
```

**Stage 2 - Subchain Removal**:
```python
def is_subchain(chain1, chain2):
    sig1 = tuple(v.name for v in chain1)
    sig2 = tuple(v.name for v in chain2)
    # Check if sig1 is contiguous in sig2
    for i in range(len(sig2) - len(sig1) + 1):
        if sig2[i:i+len(sig1)] == sig1:
            return True
    return False

# Remove if subchain exists
for chain in chains:
    if not any(is_subchain(chain, other) for other in chains):
        final_chains.append(chain)
```

**Result**: 55 unique ‚Üí 44 final chains

#### 8. Report Generator (`graph_visualizer.py`)
**Purpose**: Generate interactive HTML and JSON reports

**HTML Report Features**:
- Interactive network graph (vis.js)
- Chain details table (sortable by risk score)
- Vulnerability metadata (CWE, CVSS, URLs)
- Color-coded severity levels

**JSON Export**:
```json
{
  "chains": [
    {
      "vulnerabilities": ["XSS", "Info Disclosure", "CSRF"],
      "risk_score": 87,
      "confidence": 0.68,
      "length": 3
    }
  ]
}
```

### Data Flow Example

**Input**: http://testphp.vulnweb.com

1. **ZAP Scan** ‚Üí 129 alerts (JSON)
2. **Parser** ‚Üí 74 vulnerabilities (deduped)
3. **Taxonomy** ‚Üí Categorized (XSS, SQLi, etc.)
4. **Graph Builder** ‚Üí 74 nodes, 5,325 edges
5. **Rule Engine** ‚Üí Edges weighted by probability
6. **DFS Detector** ‚Üí 37,000 raw chains found
7. **Smart Filter** ‚Üí 44 final chains
8. **Report Generator** ‚Üí HTML + JSON output

**Total Time**: 0.4 seconds

### Chain Detection Algorithm

1. **Graph Construction**: Vulnerabilities as nodes, probabilistic rules as edges
2. **DFS Exploration**: Find all paths with length 2-4, probability ‚â•0.65
3. **Smart Limiting**: Max 500 chains per node, 100 unique patterns total
4. **Deduplication**: Remove duplicate signatures
5. **Subchain Removal**: Filter A‚ÜíB if A‚ÜíB‚ÜíC exists
6. **Risk Scoring**: Multi-factor scoring based on severity, exploitability, impact

#### Detailed Process Explanation

**Step 1: Node Creation (129 alerts ‚Üí 74 nodes)**
- **Deduplication**: ZAP finds 129 vulnerability instances
- **Signature-based grouping**: `signature = alert_name + url_base`
- **Example**: 3 XSS on `/search?q=X` ‚Üí 1 node "XSS on /search"
- **Result**: 74 unique vulnerability nodes

**Step 2: Edge Creation (74 nodes ‚Üí 5,325 edges)**
- **Rule Matching**: For each pair of nodes, check 24 chain rules
- **Probability Assignment**: Rules contain hardcoded probabilities (domain knowledge)
  - XSS ‚Üí Info Disclosure: p=0.85
  - SQLi ‚Üí Privilege Escalation: p=0.90
  - Weak Password ‚Üí SQLi: p=0.70
- **Combinatorial Explosion**: 74 √ó 73 = 5,402 possible pairs ‚Üí 5,325 matched rules
- **Note**: Probabilities are NOT calculated, they're defined by security experts

**Step 3: DFS Chain Search (5,325 edges ‚Üí 37,000 raw chains)**
- **Depth-First Search**: Recursive algorithm exploring all paths
- **Algorithm**:
  ```python
  def dfs(current, path, visited):
      if 2 ‚â§ len(path) ‚â§ 4:
          save_chain(path)
      for neighbor in graph.neighbors(current):
          if neighbor not in visited:
              dfs(neighbor, path + [neighbor], visited | {neighbor})
  ```
- **Example Path Discovery**:
  - Start: V2 (XSS)
  - Found: V2‚ÜíV3 (length 2) ‚úì
  - Found: V2‚ÜíV3‚ÜíV5 (length 3) ‚úì
  - Found: V2‚ÜíV3‚ÜíV5‚Üí... (length 4) ‚úì
- **Problem**: 74 start nodes √ó 72 avg paths = ~37,000 chains

**Step 4: On-the-fly Deduplication (37,000 ‚Üí 55 unique)**
- **Hash-based signatures**: `signature = tuple(vuln.name for vuln in chain)`
- **Duplicate detection**: If signature exists, keep chain with higher risk_score
- **Result**: 37,000 raw chains ‚Üí 55 unique patterns

**Step 5: Subchain Removal (55 ‚Üí 44 final)**
- **Logic**: If chain A‚ÜíB‚ÜíC exists, remove shorter chains A‚ÜíB and B‚ÜíC
- **Implementation**: Check if chain1 is contiguous subsequence of chain2
- **Result**: Only maximal chains remain (44 final)

**Step 6: Risk Score Calculation**
- **Formula**:
  ```python
  risk_score = (chain_length √ó 15) + (max_cvss √ó 7) + (chain_probability √ó 10)
  ```
- **Example** (V2‚ÜíV3‚ÜíV5):
  - Length: 3 ‚Üí 3 √ó 15 = 45
  - Max CVSS: 7.3 ‚Üí 7.3 √ó 7 = 51.1
  - Probability: 0.85 √ó 0.80 = 0.68 ‚Üí 0.68 √ó 10 = 6.8
  - **Total**: 45 + 51.1 + 6.8 = 102.9 (normalized to 87/100)

**Why Longer Chains = Higher Risk?**
- **Academic Perspective**: Longer chains show systemic vulnerabilities
- **Cumulative Impact**: More vulnerabilities = larger attack surface
- **Research Value**: Complex chains are more interesting for publication
- **Note**: In practice, shorter chains are easier to exploit (fewer steps)

**Impact of Changing Risk Formula:**

If we change the formula to prioritize shorter chains:
```python
# NEW FORMULA (Practical Focus)
risk_score = ((5 - chain_length) √ó 15) + (max_cvss √ó 7) + (chain_probability √ó 10)
```

**Effects:**
1. **Chain Ranking Changes**:
   - Old: V2‚ÜíV3‚ÜíV5 (length 3) = 87/100 (TOP)
   - New: V2‚ÜíV3‚ÜíV5 (length 3) = 82/100
   - New: V2‚ÜíV5 (length 2) = 97/100 (TOP) ‚Üê Direct XSS‚ÜíCSRF now prioritized

2. **Report Output Changes**:
   - Different chains highlighted in HTML report
   - Risk scores recalculated for all chains
   - Top 10 critical chains list reordered

3. **What Stays the Same**:
   - Graph structure (74 nodes, 5,325 edges) - unchanged
   - Chain discovery (37,000 ‚Üí 44 final) - unchanged
   - Probabilities (p=0.85, etc.) - unchanged
   - Deduplication logic - unchanged

4. **Files Affected**:
   - `vulnerability_chains/models.py` - VulnerabilityChain.calculate_risk_score()
   - `web_ui_app.py` - Risk score display in UI
   - HTML reports - Chain ordering by risk

5. **Recommendation for Q2 Paper**:
   - **Keep current formula** (longer = higher) for academic publication
   - Add discussion section explaining both perspectives
   - Provide alternative formula as "future work" for practical deployments

### 24 Chain Rules

**CRITICAL**: These probabilities are **hardcoded by domain experts**, NOT calculated from data!

| Rule | Example | Probability | Justification |
|------|---------|-------------|---------------|
| XSS ‚Üí CSRF | Steal CSRF token via XSS | 0.85 | Based on OWASP Top 10 patterns |
| SQLi ‚Üí Privilege Escalation | Admin access via database | 0.90 | Common in CVE database |
| Info Disclosure ‚Üí Auth Bypass | Credentials leak ‚Üí login | 0.80 | Penetration testing experience |
| SSRF ‚Üí Internal Network Access | Access internal services | 0.75 | Cloud security research |
| File Upload ‚Üí RCE | Upload malicious script | 0.95 | Nearly guaranteed if upload exists |
| XXE ‚Üí SSRF | External entity to internal | 0.85 | XML parser behavior |
| Path Traversal ‚Üí Source Disclosure | Read sensitive files | 0.80 | Common misconfiguration |
| Weak Auth ‚Üí Brute Force | Crack weak passwords | 0.70 | Depends on rate limiting |

**Source of Probabilities**:
- OWASP Top 10 attack pattern analysis
- Manual review of 100+ CVE reports
- Personal penetration testing experience
- **NO empirical validation** - this is a known limitation!

*Full configuration in `vulnerability_chains/config/chain_rules.json`*

**How Rules Work**:
1. Each rule defines: `from_category` ‚Üí `to_category` with probability
2. During graph construction, system checks if vuln1.category matches `from_category` AND vuln2.category matches `to_category`
3. If match ‚Üí add directed edge with probability weight
4. Example: If XSS vulnerability exists AND Info Disclosure exists ‚Üí add edge "XSS ‚Üí Info" with p=0.85

**Why This Approach**:
- Quick to implement for research prototype
- Allows domain knowledge integration
- Easy to modify rules without retraining
- **But**: Not data-driven, not adaptable to context

## Configuration

### Web UI Settings
- **Port**: 8888 (configurable in `web_ui_app.py`)
- **ZAP API**: localhost:8090, key "changeme"
- **Thresholds**: min_probability=0.75, min_chain_probability=0.65
- **Max Chain Length**: 4 vulnerabilities per chain

### Chain Detection Parameters
```python
# Old-style config (enhanced_detector.py)
config = {
    'min_probability': 0.75,           # Edge probability threshold
    'enable_transitive': False,        # Disable transitive links
    'enable_cluster_links': False,     # Disable cluster edges
}

# NEW: Rule Engine Configuration (probabilistic_rules.py)
from vulnerability_chains.rules.probabilistic_rules import RuleEngineConfig

rule_config = RuleEngineConfig(
    min_link_probability=0.3,          # Minimum probability for link creation (default: 0.3)
    max_boost_multiplier=2.5,          # Cap cumulative boost to prevent overflow (default: 2.5)
    enable_semantic_similarity=True,   # Use taxonomy-based similarity (default: True)
    enable_taxonomy_matching=True      # Enforce category matching (default: True)
)

# Usage
from vulnerability_chains.core.taxonomy import VulnerabilityTaxonomy
from vulnerability_chains.rules.probabilistic_rules import ProbabilisticRuleEngine

taxonomy = VulnerabilityTaxonomy()
rule_engine = ProbabilisticRuleEngine(taxonomy, config=rule_config)
```

**Configuration Benefits**:
- **Tunable precision-recall**: Adjust `min_link_probability` to balance false positives vs coverage
- **Mathematical soundness**: `max_boost_multiplier` prevents probability overflow (probabilities >1)
- **Flexibility**: Disable semantic similarity or taxonomy matching for faster processing
- **Debugging**: All intermediate values tracked in metadata

## Performance

**Important Note**: Performance metrics refer ONLY to graph analysis time, not total scanning time.

**Tested Results:**
- **ZAP Scan Time**: 8 hours (for testphp.vulnweb.com)
- **Graph Analysis Time**: 0.4 seconds
- **Total End-to-End**: ~8 hours

**Graph Analysis Breakdown**:
- Input: 129 ZAP alerts
- After deduplication: 74 unique vulnerabilities (nodes)
- Graph construction: 5,325 edges (probabilistic rules)
- DFS exploration: ~37,000 raw chains found
- After on-the-fly deduplication: 55 unique patterns
- After subchain removal: 44 final chains
- Memory: ~500MB peak

**Optimization Features:**
- **Precompiled regex patterns**: 10-100√ó speedup for URL parsing (version detection, IDOR patterns)
- **Bounded boosting**: Cap at 2.5√ó prevents computational overflow
- On-the-fly deduplication
- Per-node chain limiting (500 max)
- Early termination at 100 unique patterns
- Disabled cluster links to reduce edge explosion

## API Usage

### Python API
```python
from vulnerability_chains.core.enhanced_detector import EnhancedChainDetector
from vulnerability_chains.core.taxonomy import VulnerabilityTaxonomy
from vulnerability_chains.rules.probabilistic_rules import ProbabilisticRuleEngine
from vulnerability_chains.utils.zap_parser import ZAPAlertParser

# Parse ZAP report
parser = ZAPAlertParser()
vulnerabilities = parser.parse_zap_report('scan.json')

# Initialize detector
taxonomy = VulnerabilityTaxonomy()
rule_engine = ProbabilisticRuleEngine(taxonomy)
detector = EnhancedChainDetector(taxonomy, rule_engine, config={
    'min_probability': 0.75,
    'enable_cluster_links': False,
})

# Build graph and find chains
detector.build_graph(vulnerabilities)
chains = detector.find_chains(
    min_length=2,
    max_length=4,
    min_chain_probability=0.65
)

# Analyze results
for chain in chains:
    print(f"Risk: {chain.risk_score}, Confidence: {chain.confidence}")
    for vuln in chain.vulnerabilities:
        print(f"  - {vuln.name}")
```

### REST API Endpoints
```bash
# Start scan
POST /api/scan/start
{
  "url": "http://target.com",
  "scan_type": "active",
  "depth": 5,
  "timeout": 28800
}

# Analyze chains
POST /api/chains/analyze
{
  "scan_id": "uuid",
  "min_probability": 0.75,
  "min_chain_probability": 0.65,
  "max_chain_length": 4
}

# Get previous scans
GET /api/scans/previous

# Load scan
POST /api/scan/load/{scan_id}
```

## Requirements

- Python 3.11+
- OWASP ZAP (Docker or Desktop)
- FastAPI
- NetworkX
- Pydantic
- aiohttp

See `requirements.txt` for complete list.

## Troubleshooting

### ZAP Connection Issues
```bash
# Check ZAP is running
curl http://localhost:8090/JSON/core/view/version/?apikey=changeme

# Restart ZAP if needed
docker restart <zap-container>
```

### Chain Analysis Stuck
- Check Web UI logs: `/tmp/webui.log`
- Verify scan file exists in `scans/` directory
- Reduce thresholds if no chains found

### Too Many Chains
- Increase `min_probability` (0.75 ‚Üí 0.85)
- Increase `min_chain_probability` (0.65 ‚Üí 0.75)
- Reduce `max_chain_length` (4 ‚Üí 3)

## Project Status

**Current Version**: 2.0.0

**Completed:**
- ‚úÖ Core chain detection algorithm
- ‚úÖ ZAP integration (Docker + API)
- ‚úÖ Web-based interface
- ‚úÖ Probabilistic rule engine
- ‚úÖ Smart deduplication and filtering
- ‚úÖ HTML/JSON report generation
- ‚úÖ Performance optimization (37k ‚Üí 44 chains)

**Recent Improvements (December 2025):**

**Performance Optimizations:**
- **Taxonomy caching**: LRU cache for `classify()` ‚Üí **1291√ó speedup** (61ms ‚Üí 0.05ms for 1000 calls)
  - Eliminates redundant fuzzy matching
  - 2000-entry cache with automatic eviction
  - Cache hit rate: ~95% in typical workloads
- **Memory optimization**: Parser no longer copies entire alert dict for each instance ‚Üí **80% memory reduction**
  - Was: copying 5KB alert √ó 100 instances = 500KB
  - Now: selective field merging = 100KB
- **Precompiled regex patterns**: 10-100√ó speedup for URL parsing operations

**Rule Engine Improvements:**
- **Configurable rule engine**: Added RuleEngineConfig for tunable parameters
- **Bounded boosting**: Cap cumulative multipliers at 2.5√ó for mathematical soundness
- **Enhanced metadata**: Track all intermediate probability calculations for debugging
- **Error tracking**: Added error_stats counter for debugging boost/penalty factor failures
- **Rule consolidation**: 56 ‚Üí 53 rules (merged duplicates: XSS‚ÜíSession, Rate Limit)

**Risk Scoring Improvements:**
- **Normalized risk scores (0-100)**: Industry-standard scale replacing unbounded multiplicative scores
  - OLD: scores 420-110 (all classified as CRITICAL)
  - NEW: scores 0-100 with proper severity distribution
- **Component-based scoring**: Transparent formula with 4 factors
  - Base Severity (30%): Maximum vulnerability risk in chain
  - Exploitability (30%): Average link confidence/exploitability
  - Chain Length (20%): Logarithmic scaling (not linear)
  - Confidence (20%): Detection probability
- **Severity classification**: CRITICAL (90-100), HIGH (70-89), MEDIUM (40-69), LOW (0-39)
- **Noise filtering**: Automatic removal of non-exploitable findings
  - "ZAP is Out of Date", "Timestamp Disclosure", etc.
  - Reduces false positives in chain detection
- **Smart chain filtering**: Quality over quantity
  - Filters info-only chains (no exploitable vulnerabilities)
  - Requires HIGH risk vuln for short chains (2-3 hops)
  - Allows longer chains (4+) with MEDIUM+ risk if not all informational

**Code Quality:**
- **Dead code removal**: Deleted obsolete chain_rules.py/json (1000+ lines)
- **Unit tests**: Added test_taxonomy_cache.py with 5 test cases
- **Cache monitoring**: Added `get_cache_info()` and `clear_cache()` methods
- **Lazy logging**: Replaced f-strings with % formatting in logger calls for better performance
  - Prevents unnecessary string construction when logging is disabled
  - Fixed in probabilistic_rules.py (4 debug statements)

**System Stability:**
- Fixed chain explosion (76,480 ‚Üí 44 chains)
- Added on-the-fly deduplication
- Disabled cluster links for performance
- Implemented subchain removal
- Added previous scan loading

## License

MIT License - See LICENSE file

## Technical Details

**Algorithm Complexity:**
- Graph construction: O(n¬≤) where n = vulnerabilities
- Chain detection: O(e √ó d) where e = edges, d = max depth
- Deduplication: O(c) where c = chains found

**Optimization Strategies:**
1. **Taxonomy caching** (LRU cache, 1291√ó speedup for classification)
2. **Memory-efficient parsing** (no alert dict copying, 80% memory reduction)
3. **Precompiled regex patterns** (10-100√ó speedup for URL operations)
4. **Bounded boosting** with configurable cap (default 2.5√ó)
5. Limit chains per node (500 max)
6. Stop at unique pattern threshold (100)
7. Disable cluster links (reduces edges by 50%)
8. Early termination on duplicate signatures
9. Subchain filtering for final output
10. Configurable thresholds for precision-recall tuning

---

## Limitations and Future Work

### Current Limitations

#### 1. **Hardcoded Probabilities** ‚ö†Ô∏è Partially Addressed

**Problem**: Rule probabilities (p=0.85, p=0.90) are assigned by domain experts without empirical validation.

**Issues**:
- No methodology for determining probability values
- Probabilities don't adapt to application context
- Same p=0.85 for "XSS‚ÜíCSRF" regardless of:
  - Framework version (Angular vs jQuery)
  - Server configuration
  - Security headers presence

**Recent Improvements** ‚úÖ:
- Added **configurable thresholds** (min_link_probability) for tuning
- Implemented **bounded boosting** (max_boost_multiplier) for mathematical soundness
- Added **detailed metadata** tracking all probability calculations
- Can now adjust sensitivity without modifying rule code

**Remaining Work**:
- Collect real-world exploit chains from CVE/Exploit-DB
- Calculate empirical probabilities from historical data
- Implement Bayesian inference for context-aware probabilities
- Add machine learning model trained on actual attack patterns

#### 2. **Risk Scoring Formula Limitations** ‚úÖ RESOLVED

**OLD Formula** (before December 2025):
```python
risk_score = base_risk * link_multiplier * length_bonus * probability * 10
# Resulted in: scores 420-110 (unbounded, all CRITICAL)
```

**Problems FIXED**:
- ‚ùå Unbounded scores (420+) ‚Üí ‚úÖ Normalized 0-100
- ‚ùå All chains classified as CRITICAL ‚Üí ‚úÖ Proper severity distribution
- ‚ùå Opaque multiplicative formula ‚Üí ‚úÖ Transparent component-based scoring
- ‚ùå Linear length bonus (longer = higher) ‚Üí ‚úÖ Logarithmic scaling

**NEW Formula** (December 2025):
```python
# Component-based scoring (0-100)
risk_score = base_severity(30%) + exploitability(30%) + length(20%) + confidence(20%)

where:
  base_severity = (max_vuln_risk / 3.0) * 30    # MAX risk, not sum
  exploitability = avg_link_exploitability * 30  # Mean, not product
  length = [0,10,15,18,20][min(len-1,4)]        # Logarithmic
  confidence = chain_probability * 20

# Severity thresholds:
# CRITICAL: 90-100, HIGH: 70-89, MEDIUM: 40-69, LOW: 0-39
```

**Benefits**:
- Industry-standard 0-100 scale (like CVSS)
- Each component has clear weight (30-30-20-20)
- Logarithmic length scaling (complexity ‚â† risk)
- Transparent and debuggable

**Future Work**:
- Validate formula against penetration test results
- Add exploitability metrics (CVSS Temporal Score)
- Consider attacker skill level required
- A/B test different formulas with security professionals

#### 3. **Graph Scalability Issues**
**Current Performance**:
- 74 nodes ‚Üí 5,325 edges (O(n¬≤) complexity)
- At 200 vulnerabilities ‚Üí ~40,000 edges ‚Üí performance degradation
- At 1000 vulnerabilities ‚Üí system would hang

**Current "Optimization"**:
```python
max_chains_per_node = 500  # Simply truncate results
max_unique_patterns = 100  # Hide the problem
```

**This is not optimization, it's limitation hiding!**

**Future Work**:
- Implement graph pruning (remove low-probability edges)
- Use PageRank to prioritize high-value nodes
- Incremental analysis (don't rebuild entire graph)
- Distributed computing for large graphs
- Approximate algorithms for chain detection

#### 4. **Naive Deduplication**
**Current Logic**:
```python
signature = tuple(v.name for v in chain)
```

**Problem**: Only compares vulnerability names, ignoring:
- URLs (/login vs /search)
- Parameters (id= vs user=)
- Context (authenticated vs public)

**Example**: "XSS on /login" and "XSS on /search" treated as identical

**Future Work**:
- Include URL in signature
- Consider parameter names
- Context-aware deduplication (session state, user role)

#### 5. **Subchain Removal May Hide Critical Paths**
**Current Logic**: If A‚ÜíB‚ÜíC exists, remove A‚ÜíB

**Problem**:
- A‚ÜíB might be easier to exploit (2 steps vs 3)
- Attackers take shortest path, not longest
- System prioritizes complex chains over practical ones

**Example**:
- Removed: XSS‚ÜíCSRF (2 steps, easy)
- Kept: XSS‚ÜíInfo‚ÜíCSRF (3 steps, harder)

**Future Work**:
- Keep both short and long chains
- Add "exploitability score" to differentiate
- Provide multiple attack scenarios (quick vs thorough)

#### 6. **No Chain Validation**
**Current Behavior**: Chains are built **theoretically** from graph traversal

**Missing**:
- Practical validation (does this chain actually work?)
- Proof-of-concept generation
- False positive filtering

**Potential False Positives**:
- CSRF ‚Üí SQLi (technically impossible)
- Info Disclosure ‚Üí RCE (requires specific conditions)

**Future Work**:
- Integration with Metasploit for auto-exploitation
- PoC generation for each chain
- Manual validation mode for security analysts
- Feedback loop from confirmed exploits

#### 7. **Limited Rule Coverage**
**Current**: 24 hardcoded rules

**Missing**:
- Race conditions
- Business logic flaws
- Advanced attack patterns (Deserialization ‚Üí SSRF ‚Üí Cloud Metadata)
- Application-specific chains

**Future Work**:
- Expand to 100+ rules
- Community-contributed rules
- ML-based rule discovery from CVE database
- Dynamic rule generation based on application stack

#### 8. **ZAP False Positives Propagation**
**Problem**: ZAP is known for false positives

**Current System**:
- Accepts all 129 ZAP alerts
- Builds chains from potentially non-existent vulnerabilities
- Result: "Chains of ghosts"

**Future Work**:
- Manual confirmation step
- Integration with additional scanners (Burp, Nuclei)
- Confidence scoring for individual vulnerabilities
- Filter low-confidence alerts before graph construction

#### 9. **Performance Metrics Misleading**
**Claimed**: "0.4 seconds for 129 vulnerabilities"

**Reality**:
- ZAP scan: **8 hours**
- Graph analysis: 0.4 seconds
- **Total time**: 8 hours + 0.4 seconds

**Actual Bottleneck**: ZAP scanning, not chain detection

**Future Work**:
- Report total end-to-end time
- Optimize ZAP configuration
- Parallel scanning for large applications
- Incremental updates (scan new pages only)

#### 10. **No Comparative Evaluation**
**Missing**:
- Comparison with manual pentesting
- Benchmark against commercial tools
- Precision/Recall metrics
- Ground truth dataset

**Future Work (Phase 2 of Research)**:
- Test on DVWA, WebGoat, Juice Shop, OWASP Benchmark
- Create ground truth chains (known exploits)
- Calculate Precision, Recall, F1-score
- Compare with baseline (ZAP alone)

### Recommendations for Production Use

**DO NOT USE** in current state for:
- Critical infrastructure
- Production security assessments
- Compliance audits

**SUITABLE FOR**:
- Academic research
- Proof-of-concept demonstrations
- Educational purposes
- Baseline for further development

### Proposed Improvements Roadmap

**Phase 1: Data-Driven Probabilities** (3 months)
- Collect 1000+ real exploit chains from CVE
- Train probabilistic model
- Validate against penetration test results

**Phase 2: ML-Based Detection** (6 months)
- Replace rule engine with neural network
- Train on labeled attack chains
- Implement active learning from analyst feedback

**Phase 3: Validation Framework** (3 months)
- Auto-generate PoC exploits
- Integration with Metasploit
- Manual confirmation workflow

**Phase 4: Scalability** (4 months)
- Graph pruning algorithms
- Distributed computing
- Support for 1000+ vulnerabilities

**Total Estimated Effort**: 16 months for production-ready system

---

## Summary for New AI Assistant / Contributor

**Quick Context**:
- **What**: Graph-based vulnerability chain detection system
- **Purpose**: Q2 academic paper, NOT production tool
- **Status**: Working prototype with known limitations
- **Tech Stack**: Python 3.11, FastAPI, NetworkX, OWASP ZAP
- **Core Idea**: Find attack chains (A‚ÜíB‚ÜíC) that are more dangerous than individual vulnerabilities

**Architecture in 3 Lines**:
1. ZAP scans target ‚Üí 129 alerts
2. System builds graph (74 nodes, 5,325 edges) using 24 hardcoded rules
3. DFS finds chains ‚Üí filter to 44 final chains with risk scores

**Key Numbers**:
- 129 raw vulnerabilities ‚Üí 74 unique nodes (deduplication)
- 74 nodes ‚Üí 5,325 edges (O(n¬≤) rule matching)
- 37,000 raw chains ‚Üí 55 unique ‚Üí 44 final (aggressive filtering)
- 0.4 seconds (graph analysis) + 8 hours (ZAP scan) = total time

**Critical Trade-offs**:
1. **Hardcoded probabilities** vs data-driven (chose hardcoded for speed)
2. **Longer chains = higher risk** vs shorter = easier exploit (chose academic perspective)
3. **Subchain removal** hides practical attacks (intentional for cleaner results)
4. **No validation** of chains (theoretical only, not tested)

**Main Limitations**:
- Graph explodes at O(n¬≤) - won't scale beyond 200 vulnerabilities
- Probabilities are guesses (p=0.85), not empirical
- Risk formula uses magic numbers (15, 7, 10)
- Deduplication ignores URL context
- No proof chains actually work

**What Makes This Interesting** (for paper):
- Novel graph-based approach to chain detection
- Smart filtering reduces 37,000 ‚Üí 44 chains
- Demonstrates feasibility of automated chain discovery
- Performance: 0.4s for graph analysis
- Open limitations discussed honestly

**If Continuing This Work**:
1. Read "Limitations and Future Work" section
2. Don't break O(n¬≤) graph construction without redesign
3. Don't change risk formula without justification
4. Focus on Phase 2: empirical validation on benchmarks
5. Consider ML-based approach for Phase 3

**Files to Understand First**:
- `README.md` (this file) - full context
- `web_ui_app.py:520-588` - smart filtering logic
- `vulnerability_chains/core/enhanced_detector.py:313-372` - DFS algorithm
- `vulnerability_chains/config/chain_rules.json` - 24 rules
- `experiments/figure_*.puml|dot` - methodology diagrams

**Contact**: This is an academic research project. See LICENSE for usage terms.

---

**Last Updated**: 2025-12-21
**Status**: Research Prototype (Not Production Ready)
**Version**: 2.0.0
